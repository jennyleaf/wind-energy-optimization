# -*- coding: utf-8 -*-
"""optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DQ1jmQGi1hiGy6XimpaE_-gN7tp7iNDT

# Average stepwise variability model
"""

import pandas as pd
import numpy as np
from gurobipy import Model, GRB, quicksum
from pathlib import Path
import re

# ------------------
# Configuration
# ------------------
DATA_DIR = Path('.') 
CANDIDATES_CSV = DATA_DIR / 'TX_Substations.csv'
WIND_MATRIX_CSV = DATA_DIR / 'wind_power_matrix_sitename.csv' 
ERCOT_XLSX = DATA_DIR / 'Native_Load_2020.xlsx'

# Modeling knobs
SELECT_K = 50              # number of sites to select
ALPHA_MEAN = 1.0           # weight for mean residual demand
ALPHA_VAR = 1.0            # weight for mean absolute stepwise change
CAPACITY_MW = 100.0        # default capacity
TZ_LOCAL = 'US/Central'    # ERCOT native timezone

# ------------------
# Load wind matrix
# ------------------
print('Loading wind matrix (normalized power) ...')
W_norm = pd.read_csv(WIND_MATRIX_CSV, index_col=0, parse_dates=True)
W_norm.columns = W_norm.columns.astype(str)

# ------------------
# Load candidates and build site labels that match columns of W_norm
# ------------------
print('Loading candidate sites ...')
cand = pd.read_csv(CANDIDATES_CSV)
cols = {c.lower(): c for c in cand.columns}

site_col = cols.get('name')
if site_col is None:
    raise ValueError("candidate_sites.csv must contain a 'site' column with site names (e.g. SOUTH_CLIFF_PUMP).")

cand['site'] = cand[site_col].astype(str).str.strip().str.replace(" ", "_").str.upper()

cand = cand.drop_duplicates(subset=['site'], keep='first')

# Capacity per site
for name in cand.columns:
    cand['capacity_mw'] = CAPACITY_MW

# Keep only sites that we have in wind matrix
available_sites = sorted(set(W_norm.columns.str.upper()) & set(cand['site']))
if not available_sites:
    raise ValueError('No overlapping sites between wind matrix columns and candidate_sites site labels.')

cand = cand[cand['site'].isin(available_sites)].copy()
print(f"Candidates available after intersection: {len(cand)}")

# Slice wind matrix to matching sites
W_norm = W_norm[available_sites]

# ------------------
# Load ERCOT demand and align to UTC
# ------------------
print('Loading ERCOT hourly demand ...')

# Clean ERCOT HourEnding timestamps like '01/01/2020 24:00' or with 'DST'
def clean_hourending(val):
    if pd.isna(val):
        return val
    s = str(val).strip()
    s = re.sub(r"\\s*DST", "", s)  # remove ' DST'
    if "24:00" in s:
        date_part = s.split()[0]
        # shift to next day at 00:00
        new_date = pd.to_datetime(date_part, format="%m/%d/%Y") + pd.Timedelta(days=1)
        return new_date.strftime("%m/%d/%Y 00:00")
    return s

df_load = pd.read_excel(ERCOT_XLSX)
# Normalize headers
df_load.columns = df_load.columns.astype(str).str.strip()
# Identify time and total columns
time_col = 'HourEnding'
df_load[time_col] = df_load[time_col].apply(clean_hourending)

df_load["Datetime"] = pd.to_datetime(df_load[time_col], format="%m/%d/%Y %H:%M", errors="coerce")

# Find ERCOT total column
ercot_col = None
for c in df_load.columns:
    if c.strip().upper() == 'ERCOT':
        ercot_col = c
        break
if ercot_col is None:
    raise ValueError('Could not find ERCOT total column in Native_Load_2020.xlsx')

# Clean numbers (remove commas if present)
df_load[ercot_col] = pd.to_numeric(df_load[ercot_col].astype(str).str.replace(',', ''), errors='coerce')

# Set timezone to match NREL wind data
load_series = (
    df_load[['Datetime', ercot_col]]
      .set_index('Datetime')
      .sort_index()
)
load_series.index = load_series.index.tz_localize(TZ_LOCAL, ambiguous='NaT', nonexistent='NaT').tz_convert('UTC')
load_series = load_series[ercot_col].dropna()

# ------------------
# Align wind matrix and demand to the same hourly index
# ------------------
print('Aligning time indices ...')
# Ensure wind index is timezone-aware UTC
if W_norm.index.tz is None:
    # assume already UTC timestamps without tz; localize as UTC
    W_norm.index = W_norm.index.tz_localize('UTC')

common_idx = W_norm.index.intersection(load_series.index)
W_norm = W_norm.loc[common_idx].sort_index()
D = load_series.loc[common_idx].sort_index()
print(f"Common horizon: {len(common_idx)} hours")

# test
print("Duplicates in W_norm.columns:", W_norm.columns.duplicated().sum())
print("Duplicates in cand['site']:", cand['site'].duplicated().sum())

# ------------------
# Scale normalized wind to MW by capacity per site
# ------------------
cap_map = cand.set_index('site')['capacity_mw']
W_MW = W_norm.multiply(cap_map, axis=1)

# ------------------
# Build and solve Gurobi model (average stepwise variability)
# ------------------
print('Building Gurobi model ...')
SITES = list(W_MW.columns)
TIMES = list(W_MW.index)
T_len = len(TIMES)

m = Model('wind_siting_avg_step')

# Decision variables
x = m.addVars(SITES, vtype=GRB.BINARY, name='x')          # site selection
y = m.addVars(TIMES, lb=0.0, name='y')                    # residual demand
rpos = m.addVars(TIMES, lb=0.0, name='rpos')              # positive step change
rneg = m.addVars(TIMES, lb=0.0, name='rneg')              # negative step change

# Constraints: residual demand lower and upper bounds
W_vals = W_MW.to_dict(orient='index')  # dict: time -> {site: value}
for t in TIMES:
    m.addConstr(y[t] >= float(D.loc[t]) - quicksum(W_vals[t][s] * x[s] for s in SITES), name=f'resid_lb[{t}]')
    m.addConstr(y[t] <= float(D.loc[t]), name=f'resid_ub[{t}]')

# Stepwise variability linking
for i, t in enumerate(TIMES):
    if i == 0:
        m.addConstr(rpos[t] == 0.0, name=f'rpos0')
        m.addConstr(rneg[t] == 0.0, name=f'rneg0')
    else:
        t_prev = TIMES[i-1]
        m.addConstr(y[t] - y[t_prev] == rpos[t] - rneg[t], name=f'delta[{i}]')

# Cardinality (choose exactly K sites)
m.addConstr(quicksum(x[s] for s in SITES) == SELECT_K, name='cardinality')

# Objective: mean(y) + mean(|Î”y|)
mean_y = quicksum(y[t] for t in TIMES) / T_len
mean_abs_delta = quicksum(rpos[t] + rneg[t] for t in TIMES[1:]) / (T_len - 1)
m.setObjective(ALPHA_MEAN * mean_y + ALPHA_VAR * mean_abs_delta, GRB.MINIMIZE)

print('Optimizing ...')
m.optimize()

# ------------------
# Results
# ------------------
if m.status == GRB.OPTIMAL or m.status == GRB.TIME_LIMIT:
    sel = [s for s in SITES if x[s].X > 0.5]
    print(f"Selected sites ({len(sel)}):")
    for s in sel[:20]:
        print('  ', s)
    if len(sel) > 20:
        print('  ...')

    with open("selected_sites.csv", "w") as f:
        f.write(",".join(sel))

    # Save selection with coordinates & capacity
    out = cand.loc[cand["site"].isin(sel), ["site", "capacity_mw"]].reset_index(drop=True)
    out.to_csv('selected_sites.csv', index=False)
    print('Wrote selected_sites.csv')
